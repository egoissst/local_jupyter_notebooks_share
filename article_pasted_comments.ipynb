{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3.6 install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL PARAMETERS\n",
    "CMT_LENGTH_THRESHOLD = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_detail(msid):\n",
    "    \"\"\"\n",
    "    returns detail of the entity as json retrieved from cmsgraphread1 API call\n",
    "    \"\"\"\n",
    "    entity_detail = None\n",
    "    url = 'http://cmsgraphread1.indiatimes.com/multicontentdetailswohost?q={\"type\":\"msid\",\"id\":\"' + str(msid) + '\",\"activationStatus\":{\"status\":[0,1,2,3,4]}}'\n",
    "    try:\n",
    "        json_resp = requests.get(url).json()\n",
    "        entity_detail = json_resp['entities'][0]\n",
    "    except Exception as ex:\n",
    "        print('Exception in retrieving entity detail for msid : {}'.format(msid), ex)\n",
    "    return entity_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_detail['story'], entity_detail['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    return TAG_RE.sub(' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    \"\"\"\n",
    "    clean text function used to clean article content and comment text\n",
    "    \"\"\"\n",
    "    #replacing break signifiers with full stop so as to split into sentences properly\n",
    "    string = re.sub(r\"\\n\", \" . \", string)\n",
    "    string = re.sub(r\"<br[ /]*>\", \" . \", string)\n",
    "    \n",
    "    string = remove_tags(string)\n",
    "    string = clean_html_entities(string)\n",
    "    #string = re.sub(r\"\\n\", \" \", string)\n",
    "    string = re.sub(r\"\\t\", \" \", string)\n",
    "#     string = re.sub(r\"[\\'\\`\\\"]\", \"\", string)\n",
    "    string = re.sub(r\"[()!?\\'\\`\\\"\\”\\“\\‘\\’\\′\\″\\\\\\/*$«»°@#≈≠≤≥<>]\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_entities(string):\n",
    "    \"\"\"\n",
    "    removes html entities which are sometimes copied when trying to copy from the template\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"&ldquo;|&rdquo;|&lsquo;|&rsquo;|&mdash;|&ndash;|&plusmn;|&deg;|&laquo;|&raquo;|&quot;\", \"\", string)\n",
    "    string = re.sub(r\"&ne;|&le;|&ge;|&lt;|&gt;|&asymp;|&prime;|&Prime;|&bull;\", \"\", string)\n",
    "    string = re.sub(r\"&zwj;|&zwnj;|&lrm;|&rlm;|&bdquo;|&hellip;|&permil;|&lsaquo;|&rsaquo;|&oline;|&frasl;|&ensp;|&emsp;|&thinsp;\", \"\", string)\n",
    "    string = re.sub(r\"&amp;|&nbsp;|&:cent;|&brvbar;|&brkbar;|&sect;|&uml;|&copy;|&laqu;|&not;|&reg;|&hibar;|&sup1;|&sup2;|&sup3;|\", \"\", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_copied_from_article(cmt, raw_art_body, LD_based=True, debug=False):\n",
    "    \"\"\"\n",
    "    returns the percentage of comment that was copied from the given article body\n",
    "    process - calculates by the length of the different sentences from the comment that are present in the given art_body and the cleaned art_body\n",
    "    \"\"\"\n",
    "    #sum of matched length\n",
    "    sum_match_len = 0\n",
    "\n",
    "    cleaned_art_body = clean_text(raw_art_body)\n",
    "\n",
    "    #length zero of either comment or article body\n",
    "    if(len(cmt) == 0 or len(cleaned_art_body)==0):\n",
    "        return 0.0\n",
    "    \n",
    "    cmt_split_list = split_into_sentences(cmt)\n",
    "    \n",
    "    #total length of the parts of the sentence\n",
    "    tot_len = len(''.join(cmt_split_list))\n",
    "    \n",
    "    if (tot_len<CMT_LENGTH_THRESHOLD):\n",
    "        return 0.0\n",
    "\n",
    "    #splitting by full stop or purnaviram    \n",
    "    for y in cmt_split_list:\n",
    "        ld_factor = LD_factor(y, cleaned_art_body, LD_based=LD_based, debug=debug)\n",
    "        sum_match_len = sum_match_len + ld_factor\n",
    "    \n",
    "    #to handle for those sentences that do not have anything left after cleaning and stripping and length check\n",
    "    #return (sum_match_len/len(cmt))\n",
    "    return (sum_match_len/tot_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    returns a list of sentences from the given text\n",
    "    \"\"\"\n",
    "    MIN_SENTENCE_LENGTH = 5\n",
    "    return ([x.strip() for x in re.split('[.|।]', text) if len(x.strip())>=MIN_SENTENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split_sentences_in_cmt(cmt):\n",
    "    \"\"\"\n",
    "    splits the given text into sentences by logic and prints them\n",
    "    note - not actually used in code.. for testing purposes\n",
    "    \"\"\"\n",
    "    MIN_SENTENCE_LENGTH = 5\n",
    "    cmt = clean_text(cmt)\n",
    "    for index, y in enumerate(split_into_sentences(cmt)):\n",
    "        print(\"{} : {}\".format(index+1, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LD_factor(sent, art_body, LD_based=True, debug=False):\n",
    "    \"\"\"\n",
    "    returns the contributing length of the given sentence to the overall match length\n",
    "    \"\"\"\n",
    "    #if completely found in article body   \n",
    "    if(sent in art_body):\n",
    "#         print(art_body)\n",
    "        if(debug):\n",
    "            print('exact match found for sentence')\n",
    "            print('sent : {}'.format(sent))\n",
    "        LD_factor = len(sent)\n",
    "    elif(LD_based):\n",
    "        #finds the lowest LD for the given sentence among the sentences in the article body \n",
    "        #TODO - optimize this for the case when LD found to be 0 \n",
    "        #min_LD_value = min([Levenshtein.distance(sent, x) for x in split_into_sentences(art_body)])\n",
    "\n",
    "        art_split_sentences = split_into_sentences(art_body)\n",
    "        list_ld_values = [Levenshtein.distance(sent, x) for x in art_split_sentences]\n",
    "        min_LD_value = min(list_ld_values)\n",
    "        \n",
    "        #print([(Levenshtein.distance(sent, x),x) for x in split_into_sentences(art_body)])\n",
    "        LD_factor = max(len(sent) - min_LD_value, 0)\n",
    "        \n",
    "        #to exclude the cases where multiple contexually simiar sentences contribute to a good overall score\n",
    "        if(LD_factor/len(sent)<0.8):\n",
    "            LD_factor = 0\n",
    "        else:\n",
    "            #to print the value of the closest sentence\n",
    "            if(debug):\n",
    "                print(min_LD_value, LD_factor, art_split_sentences[list_ld_values.index(min_LD_value)])\n",
    "                print(\"sent: {}\".format(sent))\n",
    "                print()\n",
    "            pass\n",
    "    else:\n",
    "        LD_factor = 0\n",
    "    return LD_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_entity_detail(72018567)['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clean_text(get_entity_detail(71470538)['subject'] + get_entity_detail(71470538)['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_copied(cmt, msid, LD_based=True, debug=False):\n",
    "    \"\"\" \n",
    "    returns percentage copied given comment text and msid of the article\n",
    "    \"\"\"\n",
    "    if (len(cmt)<CMT_LENGTH_THRESHOLD):\n",
    "        return 0.0\n",
    "    entity_detail = get_entity_detail(msid)\n",
    "    raw_art_body = (entity_detail['subject'] if ('subject' in entity_detail) else \"\") + \" \" + (entity_detail['story'] if ('story' in entity_detail) else \"\")\n",
    "    if (cmt in raw_art_body or (clean_text(cmt) in clean_text(raw_art_body))):\n",
    "        return 1.0\n",
    "    perc_copied = perc_copied_from_article(clean_text(cmt), raw_art_body, LD_based=LD_based, debug=debug)\n",
    "    return perc_copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tata Sons' Chairman Emeritus recently shared that he was an accidental startup investor.  <em>The Tata Sons' Chairman Emeritus recently shared that he was an accidental startup investor. This is what he told the BOHECO co-founder in whose company he has invested.<br /></em><br />Earlier in the week during an interview, Ratan Tata, Chairman Emeritus of Tata Sons shared that he began startup investment by accident. After retiring, he said that he \"made small token investments\" from his pocket \"in what he considered to be exciting companies.\"<br /><br />One of these companies was the Bombay Hemp Company. <br /><br />Yash Kotak, one of the co-founders of BOHECO told ET Panache, \"People like Mr. Tata don’t just build businesses, they build legacies. He had once said, 'I don’t believe in taking the right decisions. I take decisions then make them right.' This statement has quite an ironic representation of what we’ve tried to do with BOHECO.\"<br /><br />Kotak went on to share that at the very onset, he and his co-founders faced backlash from near and dear ones. \"This was because we were venturing into an uncharted business that has a stigma attached to it. It was our belief that lead us to go ahead with it and prove the critics wrong. We took the decision and pursued it hard enough to make it the right one and here we stand as India’s premier industrial hemp and medical cannabis company and with individuals such as Mr. Tata backing us has only made this ride worth it,\" he said.<br /><br />\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t<cmsembed type=\"Slideshow\" msid=\"70270275\" hostid=\"153\" title=\"Bill Gates, Ratan Tata, Jack Ma: Business Leaders Who Accepted Their Biggest Mistakes\" firstImageMsid=\"70270302\" slideCount=\"6\" ></cmsembed>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<br /><meta content=\"cms.next\" name=\"cmsei\"/>\n"
     ]
    }
   ],
   "source": [
    "entity_detail = get_entity_detail(71644780)\n",
    "raw_art_body = (entity_detail['subject'] if ('subject' in entity_detail) else \"\") + \" \" + (entity_detail['story'] if ('story' in entity_detail) else \"\")\n",
    "print(raw_art_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_copied\n",
    "perc_copied_from_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1 : निजाम मीर उस्मान अली खान ने लंदन स्थित नेटवेस्ट बैंक में 1,007,940 पाउंड करीब 8 करोड़ 87 लाख रुपये जमा कराए थे\n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "cmt_1 = \"निजाम मीर उस्मान अली खान ने लंदन स्थित नेटवेस्ट बैंक में 1,007,940 पाउंड (करीब 8 करोड़ 87 लाख रुपये) जमा कराए थे\"\n",
    "msid_1 = 71408304\n",
    "print(perc_copied(cmt_1, msid_1, True))\n",
    "print('\\n',test_split_sentences_in_cmt(cmt_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_split_sentences_in_cmt(clean_text(cmt_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_detail = get_entity_detail(72021252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_detail = get_entity_detail(72021252)\n",
    "# raw_art_body = (entity_detail['subject'] if ('subject' in entity_detail) else \"\") + \" \" + (entity_detail['story'] if ('story' in entity_detail) else \"\")\n",
    "# print(raw_art_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "import math\n",
    "\n",
    "BASE_ELASTIC_URL = None\n",
    "BATCH_SIZE_PARAM = 250\n",
    "# ITER_BUFFER = 2\n",
    "CSV_FILE_NAME = None\n",
    "DEFAULT_FIELDS = ['C_T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startEpoch = None\n",
    "endEpoch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJsonFromUrl(url_param):\n",
    "    with urllib.request.urlopen(url_param) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCsvListFromListComments(list_comments, fields_to_take = DEFAULT_FIELDS):\n",
    "    \n",
    "    csv_list = []\n",
    "    for x in list_comments:\n",
    "        if 'C_T' in x:\n",
    "            list1 = []\n",
    "            for fieldStr in fields_to_take:\n",
    "                if \".\" in fieldStr:\n",
    "                    fieldStrArr = fieldStr.split(\".\")\n",
    "                    if fieldStrArr[0] in x and fieldStrArr[1] in x[fieldStrArr[0]]:\n",
    "                        list1.append(x[fieldStrArr[0]][fieldStrArr[1]])\n",
    "                    else:\n",
    "                        list1.append(\"\")\n",
    "                elif fieldStr in x:\n",
    "                    list1.append(x[fieldStr])\n",
    "                else:\n",
    "                    list1.append(\"\")\n",
    "            csv_list.append(list1)\n",
    "            \n",
    "    return csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModifiedUrl(url, from_param, size_param, startEpoch=None, endEpoch=None):\n",
    "    result_url = url + \"&from=\" + str(from_param) + \"&size=\" + str(size_param)\n",
    "    if (startEpoch!=None and endEpoch!=None):\n",
    "        result_url = result_url + \"&sDateEpoch=\" + str(startEpoch) + \"&eDateEpoch=\" + str(endEpoch)\n",
    "    return result_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModifiedUrlDate(url, startEpoch=None, endEpoch=None):\n",
    "#     result_url = url + \"&from=\" + str(from_param) + \"&size=\" + str(size_param)\n",
    "    result_url = url\n",
    "    if (startEpoch!=None and endEpoch!=None):\n",
    "        result_url = result_url + \"&sDateEpoch=\" + str(startEpoch) + \"&eDateEpoch=\" + str(endEpoch)\n",
    "    return result_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_url_result(url, fields_to_take = DEFAULT_FIELDS, size_param = BATCH_SIZE_PARAM, file_path = CSV_FILE_NAME, force = False, to_download_till = None):\n",
    "    \n",
    "    final_url = getModifiedUrl(url, 0, size_param)\n",
    "    json_raw_response = getJsonFromUrl(final_url)\n",
    "\n",
    "    total_comment_count = json_raw_response['hits']['total']\n",
    "    \n",
    "    ELASTIC_LIMIT = 10000\n",
    "    if (to_download_till != None and to_download_till < total_comment_count):\n",
    "        total_comment_count = to_download_till\n",
    "        \n",
    "    if (total_comment_count > ELASTIC_LIMIT):\n",
    "        print(\"total_comment_count greater than 10000: %d\" %(total_comment_count))\n",
    "        if ~force:\n",
    "            print(\"aborting\")\n",
    "            return\n",
    "        else:\n",
    "            total_comment_count = ELASTIC_LIMIT\n",
    "        \n",
    "    num_iters = math.ceil(total_comment_count/size_param)\n",
    "#     num_iters = num_iters + ITER_BUFFER\n",
    "\n",
    "    print(\"total_count : %d\" %(total_comment_count))\n",
    "    print(\"num_iters : %d\" %(num_iters))\n",
    "\n",
    "    for iter_val in range(0, num_iters, 1):\n",
    "        print(\"iteration : %d\" %(iter_val))\n",
    "        from_val = iter_val * size_param\n",
    "        final_url = getModifiedUrl(url, from_val, size_param)\n",
    "        json_raw_response = getJsonFromUrl(final_url)\n",
    "        list_comments = [x['_source'] for x in json_raw_response['hits']['hits']]\n",
    "        \n",
    "        csv_list = getCsvListFromListComments(list_comments, fields_to_take)\n",
    "        yield csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNewCsv(file_path = CSV_FILE_NAME):\n",
    "    with open(file_path, 'w+') as writeFile:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_identified_and_total_V2(csv_list, file_path):\n",
    "    PERC_THRESHOLD = 0.6\n",
    "    total_count  = len(csv_list)\n",
    "    identified_count = 0\n",
    "    \n",
    "    df_list = list()\n",
    "    for elem in csv_list:\n",
    "        try:\n",
    "            perc = perc_copied(elem[1], elem[2])\n",
    "            if (perc>PERC_THRESHOLD):\n",
    "                identified_count = identified_count + 1\n",
    "                print(elem[0], perc, elem[2], elem[1])\n",
    "                #df_list.append([elem[0], perc, elem[2], elem[1], raw_art_body])\n",
    "                df_list.append([elem[0], perc, elem[2], elem[1]])\n",
    "        except Exception as ex:\n",
    "            print('Exception for c_id : {}'.format(elem[0]), ex)\n",
    "    \n",
    "    df_csv = pd.DataFrame(df_list, columns = ['c_id', 'perc', 'msid', 'C_T'])\n",
    "        \n",
    "    with open(file_path, 'a') as f:\n",
    "        df_csv.to_csv(f, index = False, header=f.tell()==0, encoding='utf-8')\n",
    "        \n",
    "    return identified_count, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_epoch(epoch_val):\n",
    "    \"\"\"\n",
    "    returns datetime according to GMT+5:30\n",
    "    \"\"\"\n",
    "    return (datetime.datetime.utcfromtimestamp(epoch_val) + datetime.timedelta(hours=5, minutes=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# channel_list_str = \"NBTO,MTO,ET,GTech,TOI\"\n",
    "channel_list_str = \"NBTO,MTO\"\n",
    "channel_list = channel_list_str.split(\",\")\n",
    "\n",
    "for channels in channel_list:\n",
    "    print(channels)\n",
    "    base_url = \"http://commentmoderator.indiatimes.com/mytimes/elasticCommentQuery?sort=desc&appKey={}&filterCommentStatus=APPROVED,REJECTED,UNVERIFIED\".format(channels)\n",
    "\n",
    "    fields_to_take = ['c_id', 'C_T', 'msid']\n",
    "\n",
    "    batch_write_size = 500\n",
    "\n",
    "    date_file = \"1_15_oct_V9\"\n",
    "    csv_file_path = '{}_{}.csv'.format(channels, date_file)\n",
    "    createNewCsv(file_path = csv_file_path)\n",
    "    \n",
    "    NET_END_TIME_EPOCH = 1571164200000\n",
    "    NUM_DAYS_IN_ONE_ITER = 0.25\n",
    "    NUM_ITERS = 15*4\n",
    "\n",
    "    # endEpoch = NET_START_TIME_EPOCH\n",
    "    startEpoch = NET_END_TIME_EPOCH\n",
    "\n",
    "    total_count = 0\n",
    "    identified_count = 0\n",
    "\n",
    "    for iter_val in range(0, NUM_ITERS, 1):\n",
    "        print(\"\\nIter_val : %d\" %(iter_val))\n",
    "        endEpoch = startEpoch\n",
    "        startEpoch = endEpoch - int(NUM_DAYS_IN_ONE_ITER*24*60*60*1000)\n",
    "\n",
    "        url = getModifiedUrlDate(base_url, startEpoch, endEpoch)\n",
    "        print(\"url : \" + url)\n",
    "        for csv_list in generator_url_result(url, fields_to_take = fields_to_take, size_param = batch_write_size, file_path = csv_file_path):\n",
    "            n_identified_count, n_total_count = get_counts_identified_and_total_V2(csv_list, csv_file_path)\n",
    "            total_count = total_count + n_total_count\n",
    "            identified_count = identified_count + n_identified_count\n",
    "    #         print(csv_list)\n",
    "\n",
    "#     print(\"\\nnet startEpoch : \", startEpoch, \"time : {}\".format(get_datetime_epoch(startEpoch)))\n",
    "#     print(\"net endEpoch : \", NET_END_TIME_EPOCH, \"time : {}\".format(get_datetime_epoch(NET_END_TIME_EPOCH)))\n",
    "    print(\"\\nnet startEpoch : {}\".format(startEpoch))\n",
    "    print(\"net endEpoch : {}\".format(NET_END_TIME_EPOCH))\n",
    "    print(channels, identified_count, total_count, identified_count/total_count)\n",
    "    print(\"\\n\\n================================================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
